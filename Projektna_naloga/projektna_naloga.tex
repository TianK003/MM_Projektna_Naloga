\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}

\definecolor{siva}{RGB}{235,235,235}
\lstdefinestyle{Matlab-Pyglike}{
    language=Matlab,
    backgroundcolor=\color{siva},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    captionpos=b,
    frame=single,
    numbers=left,
    showtabs=false,
    tabsize=2
}
\title{Search within a collection of documents\\
\normalsize{Mathematical Modelling}
}

\author{Nik Jenič, Tian Ključanin, Maša Uhan}
\date{\today}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Problem description}

In today's digital landscape, the abundance of online information poses a significant challenge known as information overload. Traditional search methods, relying heavily on exact keyword matches, often struggle to cope with this deluge of data. They fail to account for the diverse ways people express ideas, such as using synonyms and related terms, leading to incomplete or irrelevant search results.

\bigskip
\noindent This deficiency highlights the need for a more sophisticated approach that can decipher the deeper semantic relationships between words and documents. Latent Semantic Indexing (LSI) offers a solution by going beyond literal keyword matching. It builds a model that understands the conceptual connections within content, thereby improving the accuracy and comprehensiveness of information retrieval.

\bigskip
\noindent This project aims to implement an LSI-based search engine that can efficiently process and retrieve relevant documents from a collection based on user queries. 


\subsection{Approach and Methodology}

The project involves the following key steps:

\begin{enumerate}
    \item Data Collection: Gather a collection of documents to form the basis of the search engine.
    \item Implementing LSI: Develop an LSI model to analyze the relationships between words and documents in the collection.
    \item Improving the model.
\end{enumerate}


\subsubsection{Data Collection}

We are using publicly available data for analysis. This involves curating a diverse set of documents to ensure the search engine's effectiveness across different topics and domains.


\subsubsection{Implementing LSI}

Implementing LSI entails the following steps:

\begin{enumerate}
    \item Building an $A$ matrix of connections between words and documents from a document selection, where each document has its own column in the matrix and each word has its row. The element $a_{ij}$ represents the frequency of the $i$-th word in the $j$-th document.
    \item Splitting the matrix $A$ using the SVD method, where $A = U_k  S_k  V_k^T$, which only has $k$ significant singular values.
    \item Writing the query with the vector $q$ and generating a vector from the $q$ query in the document space with the formula $\hat{q} = q^T U_k S_k^{-1}$. The query should return documents for which the cosine is higher than the selected limit.
\end{enumerate}

\subsubsection{Improving the model}

The model can be improved by replacing the frequencies in matrix $A$ with more complex measurements. In general, the element of the matrix can be written as a product:
\[ a_{ij} = L_{ij} \cdot G_i \]
where $L_{ij}$ is the local measure of the importance of a word in a document, and $G_i$ is the global measure of the importance of a word.

\bigskip
\noindent In this project, we are using a scheme where the local measure of importance is given by the logarithm of the frequency $f_{ij}$ of the $i$-th word in the $j$-th document:
\[ L_{ij} = \log (f_{ij} +1) \]
and the global measure of importance is calculated using entropy:
\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]
where $n$ is the total number of documents in the collection, $p_{ij} = \frac{f_ {ij}}{g f_i}$, and $gf_i$ is the frequency of a word in the whole collection.

\bigskip
\noindent The model can be further improved by adding new documents or words withut having to recalculate the $SVD$ of the matrix $A$.

\newpage
\section{Solution}

\subsection{Non-optimized Solution}

Our initial approach involves the construction of a basic frequency matrix, which serves as the foundation for text representation within our system. We construct the matrix as described below:

\begin{itemize}
    \item \textbf{Word Collection:} Initially, we gather all unique words from the corpus, disregarding duplicates. This collection forms the basis of the rows in our matrix, with each word allocated a specific row.
    \item \textbf{Document Parsing:} Each document in the dataset is processed to extract the words it contains. These documents correspond to the columns of our matrix.
    \item \textbf{Frequency Calculation:} For each document, we count the occurrences of each word and populate the matrix accordingly. The intersection of a row and a column in the matrix holds the frequency of the word (row) in the specified document (column).
    \item \textbf{Matrix Assembly:} The complete matrix is assembled by combining the word frequencies across all documents. This matrix is then utilized to represent the text data in a structured form.
\end{itemize}

\noindent This matrix construction does not include any advanced data handling or algorithmic optimization but lays the groundwork for further processing and analysis. The simplicity of this method gives us a clear view of the text data's key elements.


\bigskip
\noindent \textbf{Code Utilization and Output:}
\noindent The code operates by taking \textcolor{red}{a directory of documents} as input and proceeds through several steps:
\begin{enumerate}
    \item \textbf{Matrix Generation:} All text documents within the \textcolor{red}{specified directory} are processed to generate the basic frequency matrix as described above.
    \item \textbf{Document Similarity Analysis:} Using Singular Value Decomposition (SVD), the system identifies and quantifies the similarity between the textual content of the documents based on the transformed matrix data.
    \item \textbf{Query Handling:} The system allows users to input a query, which is then converted into a vector. This vector is used to find documents that are most similar to the query, based on cosine similarity metrics.
\end{enumerate}
The primary outputs from this process include a list of documents ranked by their relevance to the input query. These results help identify the most pertinent documents without the need to manually sift through the entire dataset.


\subsection{Optimized Solution}

Looking further to improve our search methods, a key challenge lies in managing the influence of word frequencies. Common words can dominate search results, while rare words might disproportionately affect outcomes, even if they're informative. To solve this problem, we have implemented an optimization technique for the term-document matrix that helps ensure no single group of words skews the results too much.

\bigskip
\noindent Our optimized solution improves previous method by substituting word frequencies in our matrix with more sophisticated metrics. These include a local measure that employs logarithmic transformations of word frequencies, and a global measure derived from entropy. This approach ensures that the impact of each word on search results accurately reflects its genuine informational value while maintaining a balanced and effective retrieval system.


\subsection{Additional Improvements}
So far, our approach, while efficient, still requires recomputing the Singular Value Decomposition (SVD) each time a new document or word is added. To resolve this issue, we have implemented an update mechanism that integrates new data into the existing SVD structure without the need for full recomputation. This enhancement significantly improves the scalability and efficiency of our system, enabling more effective handling of dynamic datasets.

\subsubsection{Adding New Documents}
When a new document is added, the term-document matrix $A$ is extended by adding a new column. This column vector, denoted as $a_{\text{new}}$, contains the term frequencies for the new document. The SVD components are then updated as follows:
\begin{itemize}
    \item Compute the projection of $a_{\text{new}}$ onto the existing left singular vectors $U_k$: 
    \[
    p = U_k^T a_{\text{new}}
    \]
    \item Compute the residual vector $r$, which is the component of $a_{\text{new}}$ orthogonal to $U_k$: 
    \[
    r = a_{\text{new}} - U_k p
    \]
    \item If $r$ is approximately zero, $a_{\text{new}}$ lies in the subspace spanned by $U_k$. Otherwise, normalize it to obtain a new singular vector component: 
    \[
    r = \frac{r}{\|r\|}
    \]
\end{itemize}
The augmented matrices are then formed to incorporate these updates into the existing SVD structure:
\begin{itemize}
    \item Form the augmented left singular vectors $U_k$:
    \[
    \tilde{U}_k = [U_k \quad r]
    \]
    \item Form the augmented singular values $\Sigma_k$:
    \[
    \tilde{\Sigma}_k = \begin{bmatrix}
    \Sigma_k & p \\
    0 & \|r\|
    \end{bmatrix}
    \]
    \item Update the right singular vectors $V_k$ to include the new document:
    \[
    \tilde{V}_k = \begin{bmatrix}
    V_k & 0 \\
    0 & 1
    \end{bmatrix}
    \]
\end{itemize}
These methods ensure that our SVD representation remains accurate and scalable as new data is incorporated.

\subsubsection{Adding New Words}
Similarly, introducing new words involves extending the term-document matrix $A$ by adding a new row for $a_{\text{new}}$. The update procedure includes:
\begin{itemize}
    \item Projecting $a_{\text{new}}$ onto the existing right singular vectors $V_k$: 
    \[
    p = a_{\text{new}} V_k
    \]
    \item Computing the residual vector $r$, orthogonal to $V_k$: 
    \[
    r = a_{\text{new}} - p V_k^T
    \]
    \item If $r$ is approximately zero, $a_{\text{new}}$ lies in the subspace spanned by $V_k$. Otherwise, normalize it to obtain a new singular vector component: 
    \[
    r = \frac{r}{\|r\|}
    \]
\end{itemize}
The augmented matrices are then formed to incorporate these updates into the existing SVD structure:
\begin{itemize}
    \item Update the right singular vectors $V_k$:
    \[
    \tilde{V}_k = [V_k \quad r]
    \]
    \item Update the singular values $\Sigma_k$:
    \[
    \tilde{\Sigma}_k = \begin{bmatrix}
    \Sigma_k & 0 \\
    p^T & \|r\|
    \end{bmatrix}
    \]
    \item Update the left singular vectors $U_k$ to include the new word:
    \[
    \tilde{U}_k = \begin{bmatrix}
    U_k & 0 \\
    0 & 1
    \end{bmatrix}
    \]
\end{itemize}
These methods ensure that our SVD representation remains accurate and scalable as new data is incorporated.

\newpage
\section{Implementation}
    With our understanding of the problem and the proposed solutions, we now move to the implementation phase. This section details the technical aspects of our system, including the code structure, key functions, and the overall workflow.

    \subsection{Tools}
    The first step in implementing our system is to select the appropriate tools and programming languages. 
    
    We decided to use the programming language Python, due to its ease of use, versatility, and extensive collection of libraries. The following libraries are essential for our implementation:

    \begin{itemize}
        \item \textbf{NumPy}
        \item \textbf{Scikit-learn}
        \item \textbf{Python-Docx}
    \end{itemize}
        \bigskip
        \noindent
        \textbf{NumPy}

        \noindent
        The leading role in our implementation is played by NumPy, a fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.

        Since our system relies heavily on large matrix operations, NumPy is an essential library for handling the underlying data structures and computations.

        \bigskip
        \noindent
        \textbf{Scikit-learn}

        \noindent
        Scikit-learn is a powerful machine learning library that provides simple and efficient tools for data mining and data analysis.
        
        We utilize Scikit-learn for its implementation of the Singular Value Decomposition (SVD) algorithm, which is a key component of our Latent Semantic Indexing (LSI) model. The SVD implementation we use is called "randomized\_svd", which is particularly well-suited for large sparse datasets, like the one we are working with. This implementation allows us to efficiently find a close approximation of the truncated SVD, which is crucial for our system's performance.

        Another important feature of Scikit-learn that we leverage is their collection of machine learning datasets. The dataset we decided to use for testing and evaluation purposes is the "20 Newsgroups" dataset, which contains approximately 13,000 newsgroup documents across 20 different categories.


        \bigskip
        \noindent
        \textbf{Python-Docx}

        \noindent
        A simple library that we used for reading DOCX files. This allowed us to easily test our implementation on a smaller local dataset.
    
    \subsection{Nevem kaj kle sam pac da pokazm da se lahk preuredi}

    \newpage
    \subsection{Testing implementation}
    To ensure the accuracy and efficiency of our system, we need to conduct thorough testing, but when dealing with machine learning models, it is often tricky finding the right metrics to evaluate the model's performance. For this reason, we tested our implementation in several ways.

        \bigskip
        \noindent
        \textbf{Automatic testing}
        
        \noindent
        To achieve realively quick, reliable, and tangible results, we first implemented the following automatic testing idea:
        All of our test data is structured in the form of e-mails. For this reason, every body of text comes with a "subject" line, that being a short description on what the e-mail is about. We can use this "subject" line as a query to our system. To ensure less bias, the subject line is removed from the testing data. The system then returns the most relevant e-mails based on the query. The results are then compared to the original "subject" line, and a score is given based on the relevance of the returned e-mails. If the correct e-mail is the most similar to our query, the score is 1, else if the correct e-mail is within the top 10 most similar e-mails, we award a score equaling the similarity of the correct e-mail, that being the one, where the subject matches the subject of the e-mail.

        \bigskip
        \noindent
        \textbf{Manual testing}

        \noindent
        To further evaluate the system's performance, we conducted manual testing. This involved selecting a set of queries and manually assessing the relevance of the returned documents. This method allowed us to gain a deeper understanding of the system's performance and identify areas for improvement, as we have many different metrics to evaluate if a returned document is useful to us or not.

\newpage
\section{Results}

\subsection{Testing Methodology}
The testing methodology for our text processing system is designed to evaluate the accuracy and efficiency of document retrieval capabilities. This evaluation is critical for ensuring that the system performs optimally under various operational conditions and with different datasets. The testing process includes the following key steps:

\begin{enumerate}
    \item \textbf{Relevance and Accuracy Assessment:} For each query, the system retrieves documents based on cosine similarity measures, comparing the system’s output against expected results. The accuracy is quantified by how closely the retrieved documents match the predefined relevant documents or subjects.

    \item \textbf{Performance Scoring:} The system assigns scores based on the relevance of the retrieved documents. Full points are awarded for exact matches, while partial scores are given for documents that are close but not exact, based on their similarity metrics.

    \item \textbf{Dynamic Data Integration Tests:} \textcolor{red}{abomo to idk ?} system is tested for its ability to dynamically update the term-document matrix and SVD components when new documents are added to the corpus. This ensures that the system remains accurate and efficient as the dataset grows.
\end{enumerate}

\noindent This structured testing strategy ensures that our document retrieval system is thoroughly evaluated for accuracy, providing confidence in its capability to handle real-world applications effectively.


\subsection{Non-optimized Solution}



\subsection{Optimized Solution}

\subsection{Additional Improvements}

\newpage
\subsection{Discussion}

\newpage
\section{References and Code}

\begin{lstlisting}[style=Matlab-Pyglike]
PLACEHOLDER FOR CODE
\end{lstlisting}

\end{document}
