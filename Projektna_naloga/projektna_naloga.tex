\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}

\definecolor{siva}{RGB}{235,235,235}
\lstdefinestyle{Matlab-Pyglike}{
    language=Matlab,
    backgroundcolor=\color{siva},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    captionpos=b,
    frame=single,
    numbers=left,
    showtabs=false,
    tabsize=2
}
\title{Search within a collection of documents\\
\normalsize{Mathematical Modelling - project}
}

\author{Nik Jenič, Tian Ključanin, Maša Uhan}
\date{\today}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Problem description}

In today's digital landscape, the abundance of online information poses a significant challenge known as information overload. Traditional search methods, relying heavily on exact keyword matches, often struggle to cope with this deluge of data. They fail to account for the diverse ways people express ideas, such as using synonyms and related terms, leading to incomplete or irrelevant search results.

\bigskip
\noindent This deficiency highlights the need for a more sophisticated approach that can decipher the deeper semantic relationships between words and documents. Latent Semantic Indexing (LSI) offers a solution by going beyond literal keyword matching. It builds a model that understands the conceptual connections within content, thereby improving the accuracy and comprehensiveness of information retrieval.

\bigskip
\noindent This project aims to implement an LSI-based search engine that can efficiently process and retrieve relevant documents from a collection based on user queries. 


\subsection{Approach and Methodology}

The project involves the following key steps:

\begin{enumerate}
    \item Data Collection: Gather a collection of documents to form the basis of the search engine.
    \item Implementing LSI: Develop an LSI model to analyze the relationships between words and documents in the collection.
    \item Improving the model.
\end{enumerate}


\subsubsection{Data Collection}

We are using publicly available data for analysis. This involves curating a diverse set of documents to ensure the search engine's effectiveness across different topics and domains.


\subsubsection{Implementing LSI}

Implementing LSI entails the following steps:

\begin{enumerate}
    \item Building an $A$ matrix of connections between words and documents from a document selection, where each document has its own column in the matrix and each word has its row. The element $a_{ij}$ represents the frequency of the $i$-th word in the $j$-th document.
    \item Splitting the matrix $A$ using the SVD method, where $A = U_k  S_k  V_k^T$, which only has $k$ significant singular values.
    \item Writing the query with the vector $q$ and generating a vector from the $q$ query in the document space with the formula $\hat{q} = q^T U_k S_k^{-1}$. The query should return documents for which the cosine is higher than the selected limit.
\end{enumerate}

\subsubsection{Improving the model}

The model can be improved by replacing the frequencies in matrix $A$ with more complex measurements. In general, the element of the matrix can be written as a product:
\[ a_{ij} = L_{ij} \cdot G_i \]
where $L_{ij}$ is the local measure of the importance of a word in a document, and $G_i$ is the global measure of the importance of a word.

\bigskip
\noindent In this project, we are using a scheme where the local measure of importance is given by the logarithm of the frequency $f_{ij}$ of the $i$-th word in the $j$-th document:
\[ L_{ij} = \log (f_{ij} +1) \]
and the global measure of importance is calculated using entropy:
\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]
where $n$ is the total number of documents in the collection, $p_{ij} = \frac{f_ {ij}}{g f_i}$, and $gf_i$ is the frequency of a word in the whole collection.

\bigskip
\noindent The model can be further improved by adding new documents or words withut having to recalculate the $SVD$ of the matrix $A$.

\newpage
\section{Solution}

\subsection{Non-optimized Solution}

Our initial approach involves the construction of a basic frequency matrix, which serves as the foundation for text representation within our system. We construct the matrix as described below:

\begin{itemize}
    \item \textbf{Word Collection:} Initially, we gather all unique words from the corpus, disregarding duplicates. This collection forms the basis of the rows in our matrix, with each word allocated a specific row.
    \item \textbf{Document Parsing:} Each document in the dataset is processed to extract the words it contains. These documents correspond to the columns of our matrix.
    \item \textbf{Frequency Calculation:} For each document, we count the occurrences of each word and populate the matrix accordingly. The intersection of a row and a column in the matrix holds the frequency of the word (row) in the specified document (column).
    \item \textbf{Matrix Assembly:} The complete matrix is assembled by combining the word frequencies across all documents. This matrix is then utilized to represent the text data in a structured form.
\end{itemize}

\noindent This matrix construction does not include any advanced data handling or algorithmic optimization but lays the groundwork for further processing and analysis. The simplicity of this method gives us a clear view of the text data's key elements.


\bigskip
\noindent \textbf{Code Utilization and Output:}
\noindent The code operates by taking \textcolor{red}{a directory of documents} as input and proceeds through several steps:
\begin{enumerate}
    \item \textbf{Matrix Generation:} All text documents within the \textcolor{red}{specified directory} are processed to generate the basic frequency matrix as described above.
    \item \textbf{Document Similarity Analysis:} Using Singular Value Decomposition (SVD), the system identifies and quantifies the similarity between the textual content of the documents based on the transformed matrix data.
    \item \textbf{Query Handling:} The system allows users to input a query, which is then converted into a vector. This vector is used to find documents that are most similar to the query, based on cosine similarity metrics.
\end{enumerate}
The primary outputs from this process include a list of documents ranked by their relevance to the input query. These results help identify the most pertinent documents without the need to manually sift through the entire dataset.


\subsection{Optimized Solution}

Looking further to improve our search methods, a key challenge lies in managing the influence of word frequencies. Common words can dominate search results, while rare words might disproportionately affect outcomes, even if they're informative. To solve this problem, we have implemented an optimization technique for the term-document matrix that helps ensure no single group of words skews the results too much.

\bigskip
\noindent Our optimized solution improves previous method by substituting word frequencies in our matrix with more sophisticated metrics. These include a local measure that employs logarithmic transformations of word frequencies, and a global measure derived from entropy. This approach ensures that the impact of each word on search results accurately reflects its genuine informational value while maintaining a balanced and effective retrieval system.


\subsection{Additional Improvements}


\newpage
\section{Results}



\newpage
\subsection{Discussion}

\newpage
\section{References and Code}

\begin{lstlisting}[style=Matlab-Pyglike]
PLACEHOLDER FOR CODE
\end{lstlisting}

\end{document}
