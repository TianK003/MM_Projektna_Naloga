\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}

\definecolor{siva}{RGB}{235,235,235}
\lstdefinestyle{Matlab-Pyglike}{
    language=Matlab,
    backgroundcolor=\color{siva},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    captionpos=b,
    frame=single,
    numbers=left,
    showtabs=false,
    tabsize=2
}
\title{Search within a collection of documents\\
\normalsize{Mathematical Modelling - project}
}

\author{Nik Jenič, Tian Ključanin, Maša Uhan}
\date{\today}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Problem description}

In today's digital landscape, the abundance of online information poses a significant challenge known as information overload. Traditional search methods, relying heavily on exact keyword matches, often struggle to cope with this deluge of data. They fail to account for the diverse ways people express ideas, such as using synonyms and related terms, leading to incomplete or irrelevant search results.

\bigskip
\noindent This deficiency highlights the need for a more sophisticated approach that can decipher the deeper semantic relationships between words and documents. Latent Semantic Indexing (LSI) offers a solution by going beyond literal keyword matching. It builds a model that understands the conceptual connections within content, thereby improving the accuracy and comprehensiveness of information retrieval.

\bigskip
\noindent Developing an effective search engine that leverages LSI is crucial for enhancing the user experience and efficiency of information retrieval in the digital age. This project aims to implement an LSI-based search engine that can efficiently process and retrieve relevant documents from a collection based on user queries. 


\subsection{Approach and Methodology}

The project will involve the following key steps:

\begin{enumerate}
    \item Data Collection: Gather a collection of documents to form the basis of the search engine.
    \item Implementing LSI: Develop an LSI model to analyze the relationships between words and documents in the collection.
    \item Improving the model.
\end{enumerate}


\subsubsection{Data Collection}

\textcolor{red}{Neki zyappamo.}


\subsubsection{Implementing LSI}

Implementing LSI will entail the following steps:

\begin{enumerate}
    \item Building an $A$ matrix of connections between words and documents from a document selection, where each document has its own column in the matrix and each word has its line. The element $a_{ij}$ represents the frequency of the $i$-th word in the $j$-th document.
    \item Splitting the matrix $A$ using the SVD method, where $A = U_k  S_k  V_k^T$, which only has $k$ significant singular values.
    \item Writing the query with the vector $q$ and generating a vector from the $q$ query in the \textcolor{red}{document space ? nevem prevoda} with the formula $q^{} = q^T U_k S_k^{-1}$. The query should return documents for which the cosine is higher than the selected limit.
\end{enumerate}

\subsubsection{Improving the model}

The model can be improved by replacing the frequencies in matrix $A$ with more complex measurements. In general, the element of the matrix can be written as a product:
\[ a_{ij} = L_{ij} \cdot G_i \]
where $L_{ij}$ is the local measure of the importance of a word in a document, and $G_i$ is the global measure of the importance of a word.

\bigskip
\noindent In this project, \textcolor{red}{we will be using (NE NOČEM)} a scheme where the local measure of importance is given by the logarithm of the frequency $f_{ij}$ of the $i$-th word in the $j$-th document:
\[ L_{ij} = \log (f_{ij} +1) \]
and the global measure of importance calculated using entropy:
\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]
where $n$ is the total number of documents in the collection, $p_{ij} = \frac{f_ {ij}}{g f_i}$, and $gf_i$ is the frequency of a word in the whole collection.

\bigskip
\noindent The model can be further improved \textcolor{red}{by adding new documents or words withut having to recalculate the $SVD$ of the matrix $A$. (NEVEM KKO LEPS NAPISAT MY BAD)}

\newpage
\section{Solution}

\newpage
\section{Results}

\newpage
\subsection{Discussion}

\newpage
\section{References and Code}

\begin{lstlisting}[style=Matlab-Pyglike]
PLACEHOLDER FOR CODE
\end{lstlisting}

\end{document}
