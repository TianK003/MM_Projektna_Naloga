\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{colortbl}

\definecolor{siva}{RGB}{235,235,235}
\definecolor{code_blue}{RGB}{66, 135, 245}
\definecolor{code_comment}{RGB}{245, 173, 66}
\definecolor{code_string}{RGB}{161, 73, 201}
\lstdefinestyle{Matlab-Pyglike}{
    language=Matlab,
    backgroundcolor=\color{siva},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    captionpos=b,
    frame=single,
    numbers=left,
    showtabs=false,
    tabsize=2
}

\lstset{
    language=Python,
    breaklines=True,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{code_blue},
    stringstyle=\color{code_string},
    commentstyle=\color{code_comment},
    morecomment=[l][\color{magenta}]{\#}
}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\title{Search within a collection of documents\\
\normalsize{Mathematical Modelling}
}

\author{Nik Jenič, Tian Ključanin, Maša Uhan}
\date{\today}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Introduction}

In today's digital landscape, the abundance of online information poses a significant challenge known as information overload. Traditional search methods, relying heavily on exact keyword matches, often struggle to cope with this deluge of data. They fail to account for the diverse ways people express ideas, such as using synonyms and related terms, leading to incomplete or irrelevant search results.

\bigskip
\noindent This deficiency highlights the need for a more sophisticated approach that can decipher the deeper semantic relationships between words and documents. Latent Semantic Indexing (LSI) offers a solution by going beyond literal keyword matching. It builds a model which understands the conceptual connections within content, thus improving the accuracy and comprehensiveness of information retrieval.

\newpage
\section{Search within a collection of documents}

\bigskip
\noindent This project aims to implement an LSI-based search engine that can efficiently process and retrieve relevant documents from a collection based on user queries.

\subsection{Approach and Methodology}

The project involves the following key steps:

\begin{enumerate}
    \item Data Collection: Gather a collection of documents to form the basis of the search engine.
    \item Implementing the LSI model: Develop an LSI model to analyze the relationships between words and documents in the collection.
    \item Testing and improving the model.
\end{enumerate}


\subsubsection{Data Collection}

We are using publicly available data for analysis. This involves curating a diverse set of documents to ensure the search engine's effectiveness across different topics and domains.


\subsubsection{Implementing the LSI model}

Implementing the LSI model entails the following steps:

\begin{enumerate}
    \item Building an $A$ matrix of connections between words and documents from a document selection, where each document has its own column in the matrix and each word has its row. The element $a_{ij}$ represents the frequency of the $i$-th word in the $j$-th document.
    \item Splitting the matrix $A$ using the SVD method, where $A = U_k  S_k  V_k^T$, which only has $k$ significant singular values.
    \item Creating a query vector $q$ from the query, where each element represents the frequency of a word in the query. We only consider words that appear in the document collection.
    \item Generating a new vector from the $q$ query vector in the document space with the formula $\hat{q} = q^T U_k S_k^{-1}$. The query should return documents for which the cosine similarity value is higher than the selected limit.
\end{enumerate}

\subsubsection{Improving the model}

The model can be improved by replacing the frequencies in matrix $A$ with more complex measurements. In general, the element of the matrix can be written as a product:
\[ a_{ij} = L_{ij} \cdot G_i \]
where $L_{ij}$ is the local measure of the importance of a word in a document, and $G_i$ is the global measure of the importance of a word.

\bigskip
\noindent In this project, we are using a scheme where the local measure of importance is given by the logarithm of the frequency $f_{ij}$ of the $i$-th word in the $j$-th document:
\[ L_{ij} = \log (f_{ij} +1) \]
and the global measure of importance is calculated using entropy:
\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]
where $n$ is the total number of documents in the collection, $p_{ij} = \frac{f_ {ij}}{g f_i}$, and $gf_i$ is the frequency of a word in the whole collection.

\bigskip
\noindent The model can be further improved by adding new documents or words without having to recalculate the $SVD$ of the matrix $A$.

\newpage
\section{Solution}

\subsection{Frequency Solution}

Our initial approach involves the construction of a basic frequency matrix, which serves as the foundation for text representation within our system. We construct the matrix as described below:

\begin{itemize}
    \item \textbf{Word Collection:} Initially, we gather all unique words from the corpus, disregarding duplicates. This collection forms the basis of the rows in our matrix, with each word allocated a specific row.
    \item \textbf{Document Parsing:} Each document in the dataset is processed to extract the words it contains. These documents correspond to the columns of our matrix.
    \item \textbf{Frequency Calculation:} For each document, we count the occurrences of each word and populate the matrix accordingly. The intersection of a row and a column in the matrix holds the frequency of the word (row) in the specified document (column).
    \item \textbf{Matrix Assembly:} The complete matrix is assembled by combining the word frequencies across all documents. This matrix is then utilized to represent the text data in a structured form.
\end{itemize}

\noindent This matrix construction does not include any advanced data handling or algorithmic optimization but lays the groundwork for further processing and analysis. The simplicity of this method gives us a clear view of the text data's key elements.


\subsection{Weighted Solution}

Looking further to improve our search methods, a key challenge lies in managing the influence of word frequencies. Common words can dominate search results, while rare words might disproportionately affect outcomes, even if they're informative. To solve this problem, we have implemented an optimization technique for the term-document matrix that helps ensure no single group of words skews the results too much.

\bigskip
\noindent Our weighted solution improves previous method by substituting word frequencies in our matrix with more sophisticated metrics. These include a local measure that employs logarithmic transformations of word frequencies, and a global measure derived from entropy. This approach ensures that the impact of each word on search results accurately reflects its genuine informational value while maintaining a balanced and effective retrieval system.


\subsection{Code Utilization and Output:}
\noindent The code operates by taking a collection of documents as input and proceeds through several steps:
\begin{enumerate}
    \item \textbf{Matrix Generation:} All text documents are processed to generate the basic frequency matrix as described above.
    \item \textbf{Document Similarity Analysis:} Using Singular Value Decomposition (SVD), the system identifies and quantifies the similarity between the textual content of the documents based on the transformed matrix data.
    \item \textbf{Query Handling:} The system allows users to input a query, which is then converted into a vector. This vector is used to find documents that are most similar to the query, based on cosine similarity metrics.
\end{enumerate}
The primary outputs from this process include a list of documents ranked by their relevance to the input query. These results help identify the most pertinent documents without the need to manually sift through the entire dataset.

\subsection{Additional Improvements}
So far, our approach, while efficient, still requires recomputing the Singular Value Decomposition (SVD) each time a new document or word is added. To resolve this issue, we have implemented an update mechanism that integrates new data into the existing SVD structure without the need for full recomputation. This enhancement significantly improves the scalability and efficiency of our system, enabling more effective handling of dynamic datasets.

\subsubsection{Adding New Documents}

When adding a new document, we first create a vector $q$ for the document, where each entry represents the frequency of a word from that document. This only includes words, which have previously appeared in other documents, skipping any new words, which will be added later. This vector is then transformed into the existing document space using the formula:

\[ \hat{q} = q^T U_k S^{-1}_k \]

\noindent The vector is then appended to the matrix $V_k$:

\[ \hat{V_k} = \begin{bmatrix} V_k & \hat{q} \end{bmatrix} \]

\subsubsection{Adding New Words}

In the process of updating the SVD with new documents, a frequency vector is constructed for each new word encountered. This vector is initially populated with zeros for all existing documents, reflecting the absence of the new word in those texts. The frequency count of the new word in the new document is then recorded in the last entry of this vector. This vector $q$ is then transformed into the document space using the formula:
\[ \hat{q} = q V^T_k S_k \]

\noindent Then, we append the vector to the matrix $U_k$:
\[ \hat{U_k} = \begin{bmatrix} U_k \\ \hat{q} \end{bmatrix} \]


\newpage
\section{Implementation}
    With our understanding of the problem and the proposed solutions, we now move to the implementation phase. This section details the technical aspects of our system, including the code structure, key functions, and the overall workflow.

    \subsection{Tools}
    The first step in implementing our system is to select the appropriate tools and programming languages. 
    
    \bigskip
    \noindent We decided to use the programming language $Python$, due to its ease of use, versatility, and extensive collection of libraries. The following libraries are essential for our implementation:

    \begin{itemize}
        \item NumPy
        \item Scikit-learn
        \item Python-Docx
    \end{itemize}

        \noindent
        \subsubsection{NumPy}

        \noindent The leading role in our implementation is played by NumPy, a fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.

        \bigskip  
        \noindent Since our system relies heavily on large matrix operations, NumPy is an essential library for handling the underlying data structures and computations.

        \bigskip
        \subsubsection{Scikit-learn}

        \noindent Scikit-learn is a powerful machine learning library that provides simple and efficient tools for data mining and data analysis.
        
        \bigskip
        \noindent We utilize Scikit-learn for its implementation of the SVD algorithm, which is a key component of our LSI model. The SVD implementation we use is called "randomized\_svd", which is particularly well-suited for large sparse datasets, like the one we are working with. This implementation allows us to efficiently find a close approximation of the truncated SVD, which is crucial for our system's performance.

        \bigskip
        \noindent Another important feature of Scikit-learn that we leverage is their collection of machine learning datasets. The dataset we decided to use for testing and evaluation purposes is the "20 Newsgroups" dataset, which contains approximately 13,000 newsgroup documents across 20 different categories.


        \bigskip
        \subsubsection{Python-Docx}

        \noindent
        A simple library that we used for reading DOCX files. This allowed us to easily test our implementation on a smaller local dataset.

\subsection{Testing Implementation and Methodology}
To ensure the accuracy and efficiency of our document retrieval system, thorough testing was conducted using a combination of automatic and manual methods. These methods were designed to address both the performance and the dynamic capabilities of the system under various conditions.

\subsubsection{Automatic Testing}
\noindent
Our test data is structured in the form of emails, each including a "subject" line which serves as a concise description of the content. In our testing setup, we removed the subject line from the email to use as a query, reducing bias in testing. The system then retrieves the most relevant emails based on these queries. Results are compared to the original subject lines to assess relevance. The scoring system awards full points if the correct email is the most similar to the query. If the correct email ranks within the top ten of the most similar emails, a full point is awarded (our testing shows that similarity score may be quite low even for most similar documents, so a full point is used regardless of the similarity score).

\subsubsection{Manual Testing}
\noindent
In addition to automated tests, we manually selected queries and assessed the relevance of the returned documents. This approach allowed for a detailed evaluation of the system's effectiveness and helped identify potential areas for improvement. Different metrics were utilized to assess whether a returned document was genuinely relevant.

\subsubsection{Evaluation Criteria}
The evaluation process therefore includes several key steps to ensure comprehensive testing:

\begin{enumerate}
    \item \textbf{Relevance and Accuracy Assessment:} Documents are retrieved based on cosine similarity measures for each query. Accuracy is quantified by the closeness of the retrieved documents to predefined relevant documents or subjects.
    \item \textbf{Performance Scoring:} The system assigns scores based on the relevance of the retrieved documents. Exact matches receive full points, while partial scores are awarded for close matches, based on their similarity metrics.
    \item \textbf{Dynamic Data Integration Tests:} The system's ability to dynamically update the term-document matrix and SVD components when new documents are added is critically tested. This ensures that the system maintains its accuracy and efficiency as the dataset grows.
\end{enumerate}

\noindent
This structured testing strategy ensures that our document retrieval system is thoroughly evaluated for accuracy and reliability.

\newpage

\section{Results}

\subsection{Automatic testing}
Here is an overview of the table structures used to present the results of our testing process. These tables provide a clear and concise summary of the system's performance under different conditions, allowing for easy comparison and analysis of the results. The tables are structured as follows:
\begin{itemize}
  \item \textbf{Rows:} Each row represents a different \( k \) value, where \( k \) is the number of biggest singular values in the SVD decomposition. The values range from 10 to 1000, allowing us to test the system's scalability across different dimensions of space.
  \item \textbf{Columns:} The columns represent different threshold values, related to the cosine similarity thresholds used in the retrieval process. These values range from 0.1 to 0.9, affecting how strict the similarity criterion is for considering two documents as closely related.
  \item \textbf{Cell Values:} Each cell in the table represents the number of points the program scored for each combination of \( k \) and cosine similarity threshold. These values illustrate how the retrieval effectiveness varies with the complexity of the model and the strictness of the similarity threshold.
\end{itemize}

The tables present the results of testing different configurations of the non-weighted and weighted solution implemented in the document retrieval system (results based on testing on 1000 documents).
\newpage

\begin{table}[h!]
    \centering
    \arrayrulecolor{black} % Set the color of the table lines to black
    \setlength{\arrayrulewidth}{0.5mm} % Set the width of the table lines
    \renewcommand{\arraystretch}{1.5} % Increasing the height of rows
    \rowcolors{2}{gray!25}{white} % Alternating row colors starting from the second row
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \rowcolor{black!75} % Header row color
    & \color{white}\textbf{0.10} & \color{white}\textbf{0.20} & \color{white}\textbf{0.30} & \color{white}\textbf{0.40} & \color{white}\textbf{0.50} & \color{white}\textbf{0.60} & \color{white}\textbf{0.70} & \color{white}\textbf{0.80} & \color{white}\textbf{0.90} \\ \hline
    \cellcolor{black!75}\color{white}\textbf{10} & 52.17 & 52.17 & 52.17 & 52.17 & 52.17 & 51.59 & 50.97 & 44.25 & 25.77 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{50} & 182.6 & 182.6 & 182.6 & 177.4 & 157.5 & 115.1 & 93.88 & 73.63 & 30 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{100} & 277.3 & 277.3 & 271.3 & 237.5 & 182.5 & 122.6 & 100.7 & 70 & 22 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{250} & 469.2 & 468.2 & 436.3 & 355.4 & 255.8 & 191.9 & 127.7 & 83 & 26 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{500} & 613.5 & 606.9 & 557.6 & 465.0 & 356.9 & 239.6 & 155 & 81 & 28 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{750} & 668.4 & 660.4 & 618.1 & 526.5 & 389.5 & 281.7 & 168 & 100 & 26 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{1000} & 641.9 & 627.2 & 580.6 & 486.4 & 385.2 & 295.6 & 213 & 119 & 37 \\ \hline
    \end{tabular}
    \caption{Non-weighted Solution for Different Values of $k$}
\end{table}

\begin{table}[h!]
    \centering
    \arrayrulecolor{black} % Set the color of the table lines to black
    \setlength{\arrayrulewidth}{0.5mm} % Set the width of the table lines
    \renewcommand{\arraystretch}{1.5} % Increasing the height of rows
    \rowcolors{2}{gray!25}{white} % Alternating row colors starting from the second row
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \rowcolor{black!75} % Header row color
    & \color{white}\textbf{0.10} & \color{white}\textbf{0.20} & \color{white}\textbf{0.30} & \color{white}\textbf{0.40} & \color{white}\textbf{0.50} & \color{white}\textbf{0.60} & \color{white}\textbf{0.70} & \color{white}\textbf{0.80} & \color{white}\textbf{0.90} \\ \hline
    \cellcolor{black!75}\color{white}\textbf{10}   & 71.18 & 71.18 & 71.18 & 71.18 & 70.72 & 68.47 & 68.47 & 63.06 & 44.12 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{50}    & 300.6 & 300.6 & 300.0 & 298.9 & 286.6 & 259.3 & 198.7 & 128.5 & 57.95 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{100}   & 442.1 & 442.1 & 441.2 & 417.0 & 354.4 & 273.8 & 180.7 & 116.5 & 58 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{250}   & 637.2 & 636.5 & 622.4 & 547.8 & 426.8 & 304.0 & 213.5 & 128.8 & 61 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{500}   & 726.5 & 722.5 & 674.8 & 576.7 & 435.6 & 325.3 & 201.0 & 102 & 65 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{750}   & 753.3 & 741.5 & 685.0 & 589.2 & 459.6 & 322.3 & 219 & 123 & 56 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{1000}  & 673.5 & 657.3 & 604.0 & 499.4 & 409.7 & 316.6 & 232 & 141 & 70 \\ \hline
    \end{tabular}
    \caption{Optimized Solution for Different Values of $k$}
\end{table}

\subsection{Manual testing}
We manually tested the weighted solution with the improvement of folding-in new documents and words. This was done on an weighted solution. This was done on a collection of 1000 documents and we slowly added more documents and terms to the system, till it's behaviour became unpredictable and the results seemed random.

\subsection{Discussion}
In this section we will discuss our findings.
\subsubsection{Comparing Original and Weighted solution}
Observing the results from both the non-weighted and weighted solutions, several key insights can be drawn:
\begin{itemize}
    \item The weighted solution consistently outperforms the non-weighted solution across all tested configurations. This improvement is most notable in scenarios with higher \( k \) values and stricter cosine similarity thresholds.
    \item The weighted solution demonstrates better scalability and robustness, maintaining higher scores as the dimensionality of the space increases.
    \item The non-weighted solution shows a decline in performance as the number of singular values \( k \) grows, indicating limitations in handling larger datasets and more complex models.
    \item Both solutions exhibit a general trend of decreasing scores with stricter cosine similarity thresholds, reflecting the trade-off between precision and recall in document retrieval.    
\end{itemize}

\bigskip
\noindent We conducted different testing for the weighted solution, one of which was changing the formula for the global measure of importance. We changed the formula from:


\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \quad \longrightarrow \quad G_i = 1 + \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]

\noindent This causes the inverse effect of the original goal, which was preventing "over-fitting", meaning putting too much focus on specific words.

\bigskip
\noindent The results showed that the weighted solution with the original formula performed better - this may be due to key words often found in the subject line in our documents.

\newpage
\subsubsection{Folding-in new documents and terms}

\noindent We have noticed that when folding-in new terms and documents, the system's performance starts declining rapidly after adding around 5\% new documents (relatively to the number of already existing ones). This is due to the fact that the new documents are not as relevant to the existing ones, and the system has a hard time finding the right documents. 

\bigskip
\noindent This is a common issue with LSI models, as they are not as effective when dealing with large amounts of new data that is not closely related to the existing data.

\bigskip
\noindent Folding-in more than 5\% new documents and/or terms there is a rapid fall off in performance, and behaviour of the system may become unpredictable. The results we got seemed random and we advise against using the system in such a state.
\newpage
\section{Conclusion}
Through making this paper, we found that LSI models are an effective tool for searching through large collections of documents, with a relatively simple implementation. We have also found that the weighted solution outperforms the non-weighted solution, though different weighing methods are worth looking into. The system can also be improved by adding new documents and terms without having to recalculate the SVD of the matrix, which could lead to long downtimes in a real-world scenario.

\bigskip
\noindent
This project helped us in the field of machine learning and natural language processing, while also reinforcing our programming and linear algebra knowledge.

\newpage
\section{References and Code}
All the code used in this project can be found on our GitHub repository: \url{https://github.com/TianK003/MM_Projektna_Naloga} under the folder "Koda".
\subsection{main.py}
\lstinputlisting[language=Python, breaklines=True]{../Koda/main.py}
\newpage
\subsection{generateMatrix.py}
\lstinputlisting[language=Python, breaklines=True]{../Koda/generateMatrix.py}
\newpage
\subsection{debug.py}
\lstinputlisting[language=Python, breaklines=True]{../Koda/debug.py}
% \begin{lstlisting}[style=Matlab-Pyglike]

% \end{lstlisting}

\end{document}
