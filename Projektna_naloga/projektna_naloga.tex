\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{matlab-prettifier}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{colortbl}

\definecolor{siva}{RGB}{235,235,235}
\lstdefinestyle{Matlab-Pyglike}{
    language=Matlab,
    backgroundcolor=\color{siva},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{orange},
    captionpos=b,
    frame=single,
    numbers=left,
    showtabs=false,
    tabsize=2
}
\title{Search within a collection of documents\\
\normalsize{Mathematical Modelling}
}

\author{Nik Jenič, Tian Ključanin, Maša Uhan}
\date{\today}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Problem description}

In today's digital landscape, the abundance of online information poses a significant challenge known as information overload. Traditional search methods, relying heavily on exact keyword matches, often struggle to cope with this deluge of data. They fail to account for the diverse ways people express ideas, such as using synonyms and related terms, leading to incomplete or irrelevant search results.

\bigskip
\noindent This deficiency highlights the need for a more sophisticated approach that can decipher the deeper semantic relationships between words and documents. Latent Semantic Indexing (LSI) offers a solution by going beyond literal keyword matching. It builds a model that understands the conceptual connections within content, thereby improving the accuracy and comprehensiveness of information retrieval.

\bigskip
\noindent This project aims to implement an LSI-based search engine that can efficiently process and retrieve relevant documents from a collection based on user queries. 


\subsection{Approach and Methodology}

The project involves the following key steps:

\begin{enumerate}
    \item Data Collection: Gather a collection of documents to form the basis of the search engine.
    \item Implementing LSI: Develop an LSI model to analyze the relationships between words and documents in the collection.
    \item Improving the model.
\end{enumerate}


\subsubsection{Data Collection}

We are using publicly available data for analysis. This involves curating a diverse set of documents to ensure the search engine's effectiveness across different topics and domains.


\subsubsection{Implementing LSI}

Implementing LSI entails the following steps:

\begin{enumerate}
    \item Building an $A$ matrix of connections between words and documents from a document selection, where each document has its own column in the matrix and each word has its row. The element $a_{ij}$ represents the frequency of the $i$-th word in the $j$-th document.
    \item Splitting the matrix $A$ using the SVD method, where $A = U_k  S_k  V_k^T$, which only has $k$ significant singular values.
    \item Writing the query with the vector $q$ and generating a vector from the $q$ query in the document space with the formula $\hat{q} = q^T U_k S_k^{-1}$. The query should return documents for which the cosine is higher than the selected limit.
\end{enumerate}

\subsubsection{Improving the model}

The model can be improved by replacing the frequencies in matrix $A$ with more complex measurements. In general, the element of the matrix can be written as a product:
\[ a_{ij} = L_{ij} \cdot G_i \]
where $L_{ij}$ is the local measure of the importance of a word in a document, and $G_i$ is the global measure of the importance of a word.

\bigskip
\noindent In this project, we are using a scheme where the local measure of importance is given by the logarithm of the frequency $f_{ij}$ of the $i$-th word in the $j$-th document:
\[ L_{ij} = \log (f_{ij} +1) \]
and the global measure of importance is calculated using entropy:
\[ G_i = 1 - \sum_{j} \frac{p_{ij} \log (p_{ij})}{\log n} \]
where $n$ is the total number of documents in the collection, $p_{ij} = \frac{f_ {ij}}{g f_i}$, and $gf_i$ is the frequency of a word in the whole collection.

\bigskip
\noindent The model can be further improved by adding new documents or words withut having to recalculate the $SVD$ of the matrix $A$.

\newpage
\section{Solution}

\subsection{Non-optimized Solution}

Our initial approach involves the construction of a basic frequency matrix, which serves as the foundation for text representation within our system. We construct the matrix as described below:

\begin{itemize}
    \item \textbf{Word Collection:} Initially, we gather all unique words from the corpus, disregarding duplicates. This collection forms the basis of the rows in our matrix, with each word allocated a specific row.
    \item \textbf{Document Parsing:} Each document in the dataset is processed to extract the words it contains. These documents correspond to the columns of our matrix.
    \item \textbf{Frequency Calculation:} For each document, we count the occurrences of each word and populate the matrix accordingly. The intersection of a row and a column in the matrix holds the frequency of the word (row) in the specified document (column).
    \item \textbf{Matrix Assembly:} The complete matrix is assembled by combining the word frequencies across all documents. This matrix is then utilized to represent the text data in a structured form.
\end{itemize}

\noindent This matrix construction does not include any advanced data handling or algorithmic optimization but lays the groundwork for further processing and analysis. The simplicity of this method gives us a clear view of the text data's key elements.


\bigskip
\noindent \textbf{Code Utilization and Output:}
\noindent The code operates by taking \textcolor{red}{a directory of documents} as input and proceeds through several steps:
\begin{enumerate}
    \item \textbf{Matrix Generation:} All text documents within the \textcolor{red}{specified directory} are processed to generate the basic frequency matrix as described above.
    \item \textbf{Document Similarity Analysis:} Using Singular Value Decomposition (SVD), the system identifies and quantifies the similarity between the textual content of the documents based on the transformed matrix data.
    \item \textbf{Query Handling:} The system allows users to input a query, which is then converted into a vector. This vector is used to find documents that are most similar to the query, based on cosine similarity metrics.
\end{enumerate}
The primary outputs from this process include a list of documents ranked by their relevance to the input query. These results help identify the most pertinent documents without the need to manually sift through the entire dataset.


\subsection{Optimized Solution}

Looking further to improve our search methods, a key challenge lies in managing the influence of word frequencies. Common words can dominate search results, while rare words might disproportionately affect outcomes, even if they're informative. To solve this problem, we have implemented an optimization technique for the term-document matrix that helps ensure no single group of words skews the results too much.

\bigskip
\noindent Our optimized solution improves previous method by substituting word frequencies in our matrix with more sophisticated metrics. These include a local measure that employs logarithmic transformations of word frequencies, and a global measure derived from entropy. This approach ensures that the impact of each word on search results accurately reflects its genuine informational value while maintaining a balanced and effective retrieval system.


\subsection{Additional Improvements}
So far, our approach, while efficient, still requires recomputing the Singular Value Decomposition (SVD) each time a new document or word is added. To resolve this issue, we have implemented an update mechanism that integrates new data into the existing SVD structure without the need for full recomputation. This enhancement significantly improves the scalability and efficiency of our system, enabling more effective handling of dynamic datasets.

\subsubsection{Adding New Words}

In the process of updating the SVD with new documents, a frequency vector is constructed for each new word encountered. This vector is initially populated with zeros for all existing documents, reflecting the absence of the new word in those texts. The frequency count of the new word in the new document is then recorded in the last entry of this vector. This vector $q$ is then transformed into the document space using the formula:
\[ \hat{q} = q V^T_k S_k \]

\noindent Then, we append the vector to the matrix $U_k$:
\[ \hat{U_k} = \begin{bmatrix} U_k \\ \hat{q} \end{bmatrix} \]

\subsubsection{Adding New Documents}

When adding a new document, we first create a vector $q$ for the document, where each entry represents the frequency of a word from that document. This only includes words, which have previously appeared in other documents, skipping any new words, which will be added later. This vector is then transformed into the existing document space using the formula:

\[ \hat{q} = q^T U_k S^{-1}_k \]

\noindent The vector is then appended to the matrix $V_k$:

\[ \hat{V_k} = \begin{bmatrix} V_k & \hat{q} \end{bmatrix} \]

\newpage
\section{Implementation}
    With our understanding of the problem and the proposed solutions, we now move to the implementation phase. This section details the technical aspects of our system, including the code structure, key functions, and the overall workflow.

    \subsection{Tools}
    The first step in implementing our system is to select the appropriate tools and programming languages. 
    
    \noindent We decided to use the programming language Python, due to its ease of use, versatility, and extensive collection of libraries. The following libraries are essential for our implementation:

    \begin{itemize}
        \item NumPy
        \item Scikit-learn
        \item Python-Docx
    \end{itemize}
        \bigskip
        \noindent
    
        \subsubsection{NumPy}

        \noindent The leading role in our implementation is played by NumPy, a fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.

        \bigskip  
        \noindent Since our system relies heavily on large matrix operations, NumPy is an essential library for handling the underlying data structures and computations.

        \bigskip
        \subsubsection{Scikit-learn}

        \noindent Scikit-learn is a powerful machine learning library that provides simple and efficient tools for data mining and data analysis.
        
        \bigskip
        \noindent We utilize Scikit-learn for its implementation of the Singular Value Decomposition (SVD) algorithm, which is a key component of our Latent Semantic Indexing (LSI) model. The SVD implementation we use is called "randomized\_svd", which is particularly well-suited for large sparse datasets, like the one we are working with. This implementation allows us to efficiently find a close approximation of the truncated SVD, which is crucial for our system's performance.

        \bigskip
        \noindent Another important feature of Scikit-learn that we leverage is their collection of machine learning datasets. The dataset we decided to use for testing and evaluation purposes is the "20 Newsgroups" dataset, which contains approximately 13,000 newsgroup documents across 20 different categories.


        \bigskip
        \subsubsection{Python-Docx}

        \noindent
        A simple library that we used for reading DOCX files. This allowed us to easily test our implementation on a smaller local dataset.
    
    \subsection{Nevem kaj kle sam pac da pokazm da se lahk preuredi}

\subsection{Testing Implementation and Methodology}
To ensure the accuracy and efficiency of our document retrieval system, thorough testing was conducted using a combination of automatic and manual methods. These methods were designed to address both the performance and the dynamic capabilities of the system under various conditions.

\subsubsection{Automatic Testing}
\noindent
Our test data is structured in the form of emails, each including a "subject" line which serves as a concise description of the content. In our testing setup, we removed the subject line from the email to use as a query, reducing bias in testing. The system then retrieves the most relevant emails based on these queries. Results are compared to the original subject lines to assess relevance. The scoring system awards full points if the correct email is the most similar to the query. If the correct email ranks within the top ten of the most similar emails, points are awarded based on its similarity score.

\subsubsection{Manual Testing}
\noindent
In addition to automated tests, we manually selected queries and assessed the relevance of the returned documents. This approach allowed for a detailed evaluation of the system's effectiveness and helped identify potential areas for improvement. Different metrics were utilized to assess whether a returned document was genuinely relevant.

\subsubsection{Evaluation Criteria}
The evaluation process therefore includes several key steps to ensure comprehensive testing:

\begin{enumerate}
    \item \textbf{Relevance and Accuracy Assessment:} Documents are retrieved based on cosine similarity measures for each query. Accuracy is quantified by the closeness of the retrieved documents to predefined relevant documents or subjects.
    \item \textbf{Performance Scoring:} The system assigns scores based on the relevance of the retrieved documents. Exact matches receive full points, while partial scores are awarded for close matches, based on their similarity metrics.
    \item \textbf{Dynamic Data Integration Tests:} The system's ability to dynamically update the term-document matrix and SVD components when new documents are added is critically tested. This ensures that the system maintains its accuracy and efficiency as the dataset grows.
\end{enumerate}

\noindent
This structured testing strategy ensures that our document retrieval system is thoroughly evaluated for accuracy and reliability.

\newpage

\section{Results}

\subsection{Non-optimized Solution}

The table below presents the results of testing different configurations of the non-optimized solution implemented in the document retrieval system. Each entry in the table corresponds to the output from a series of automated script executions that vary two main parameters: the number of documents $k$ and a threshold value.

\begin{table}[h!]
    \centering
    \arrayrulecolor{black} % Set the color of the table lines to black
    \setlength{\arrayrulewidth}{0.5mm} % Set the width of the table lines
    \renewcommand{\arraystretch}{1.5} % Increasing the height of rows
    \rowcolors{2}{gray!25}{white} % Alternating row colors starting from the second row
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \rowcolor{black!75} % Header row color
    & \color{white}\textbf{0.10} & \color{white}\textbf{0.20} & \color{white}\textbf{0.30} & \color{white}\textbf{0.40} & \color{white}\textbf{0.50} & \color{white}\textbf{0.60} & \color{white}\textbf{0.70} & \color{white}\textbf{0.80} & \color{white}\textbf{0.90} \\ \hline
    \cellcolor{black!75}\color{white}\textbf{10} & 52.17 & 52.17 & 52.17 & 52.17 & 52.17 & 51.59 & 50.97 & 44.25 & 25.77 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{50} & 182.6 & 182.6 & 182.6 & 177.4 & 157.5 & 115.1 & 93.88 & 73.63 & 30 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{100} & 277.3 & 277.3 & 271.3 & 237.5 & 182.5 & 122.6 & 100.7 & 70 & 22 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{250} & 469.2 & 468.2 & 436.3 & 355.4 & 255.8 & 191.9 & 127.7 & 83 & 26 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{500} & 613.5 & 606.9 & 557.6 & 465.0 & 356.9 & 239.6 & 155 & 81 & 28 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{750} & 668.4 & 660.4 & 618.1 & 526.5 & 389.5 & 281.7 & 168 & 100 & 26 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{1000} & 641.9 & 627.2 & 580.6 & 486.4 & 385.2 & 295.6 & 213 & 119 & 37 \\ \hline
    \end{tabular}
    \caption{Non-optimized Solution for Different Values of $k$}
\end{table}

\subsubsection{Table Structure}
\begin{itemize}
  \item \textbf{Rows:} Each row represents a different \( k \) value, where \( k \) is the number of documents. The \( k \) values range from 10 to 1000, allowing us to test the system's scalability across different document counts
  \item \textbf{Columns:} The columns represent different threshold values, related to the cosine similarity thresholds used in the retrieval process. These values range from 0.1 to 0.9, affecting how strict the similarity criterion is for considering two documents as closely related.
  \item \textbf{Cell Values:} Each cell in the table represents the number of points the program scored for each combination of \( k \) and cosine similarity threshold. These values illustrate how the retrieval effectiveness varies with the complexity of the model and the strictness of the similarity threshold.

\end{itemize}


\subsection{Optimized Solution}

\begin{table}[h!]
    \centering
    \arrayrulecolor{black} % Set the color of the table lines to black
    \setlength{\arrayrulewidth}{0.5mm} % Set the width of the table lines
    \renewcommand{\arraystretch}{1.5} % Increasing the height of rows
    \rowcolors{2}{gray!25}{white} % Alternating row colors starting from the second row
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \rowcolor{black!75} % Header row color
    & \color{white}\textbf{0.10} & \color{white}\textbf{0.20} & \color{white}\textbf{0.30} & \color{white}\textbf{0.40} & \color{white}\textbf{0.50} & \color{white}\textbf{0.60} & \color{white}\textbf{0.70} & \color{white}\textbf{0.80} & \color{white}\textbf{0.90} \\ \hline
    \cellcolor{black!75}\color{white}\textbf{10}   & 71.18 & 71.18 & 71.18 & 71.18 & 70.72 & 68.47 & 68.47 & 63.06 & 44.12 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{50}    & 300.6 & 300.6 & 300.0 & 298.9 & 286.6 & 259.3 & 198.7 & 128.5 & 57.95 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{100}   & 442.1 & 442.1 & 441.2 & 417.0 & 354.4 & 273.8 & 180.7 & 116.5 & 58 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{250}   & 637.2 & 636.5 & 622.4 & 547.8 & 426.8 & 304.0 & 213.5 & 128.8 & 61 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{500}   & 726.5 & 722.5 & 674.8 & 576.7 & 435.6 & 325.3 & 201.0 & 102 & 65 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{750}   & 753.3 & 741.5 & 685.0 & 589.2 & 459.6 & 322.3 & 219 & 123 & 56 \\ \hline
    \cellcolor{black!75}\color{white}\textbf{1000}  & 673.5 & 657.3 & 604.0 & 499.4 & 409.7 & 316.6 & 232 & 141 & 70 \\ \hline
    \end{tabular}
    \caption{Optimized Solution for Different Values of $k$}
\end{table}

\subsection{Additional Improvements}

\newpage
\subsection{Discussion}

\newpage
\section{References and Code}

\begin{lstlisting}[style=Matlab-Pyglike]
PLACEHOLDER FOR CODE
\end{lstlisting}

\end{document}
